# -*- coding: utf-8 -*-
"""Lung cancer prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KRzcF8Isz85i5rJD9XXj3AqNNrttYCpf
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Dataset/survey lung cancer.csv')

df.head()

df.columns

df.tail()

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df.describe()

df.describe(include='object')

df['GENDER'].value_counts()

for column in df.select_dtypes(include=['number']).columns:
    plt.figure(figsize=(8, 6))
    sns.histplot(df[column], kde=True)
    plt.title(f"Distribution of {column}")
    plt.show()

sns.pairplot(df, hue='LUNG_CANCER')
plt.show()

correlation_matrix = df.corr(numeric_only=True)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

for column in df.select_dtypes(include=['object']).columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x=column, y='AGE', data=df)
    plt.title(f"{column} vs. AGE")
    plt.show()

df['AGE'].plot(kind='hist', bins=20, title='AGE')
plt.gca().spines[['top', 'right',]].set_visible(False)

df['SMOKING'].plot(kind='hist', bins=20, title='SMOKING')
plt.gca().spines[['top', 'right',]].set_visible(False)

gender_counts = df['GENDER'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Gender Distribution')
plt.show()

cancer_counts = df['LUNG_CANCER'].value_counts()
plt.figure(figsize=(6, 6))
plt.pie(cancer_counts, labels=cancer_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Lung Cancer Distribution')
plt.show()

x = df.drop(columns=['LUNG_CANCER'], axis =1 )
y = df['LUNG_CANCER']

encoder = LabelEncoder()
x['GENDER'] = encoder.fit_transform(x['GENDER'])

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

models = {
    'dt': DecisionTreeClassifier(random_state=42),
    'rf': RandomForestClassifier(random_state=42),
    'lg': LogisticRegression(random_state=42),
    'knn' : KNeighborsClassifier(),
    'svm' : SVC(random_state=42),
    'nb' : GaussianNB()
}

name = ['dt', 'rf', 'lg', 'knn', 'svm', 'nb']

accuracies = []

for n in name:
    print(f'Model: {n}')
    model = models[n]
    model.fit(x_train, y_train)
    pred = model.predict(x_test)
    accuracy = model.score(x_test, y_test)*100
    accuracies.append(accuracy)
    print(classification_report(y_test, pred))
    print(confusion_matrix(y_test, pred))
    print('Accuracy: {:.2f}'.format(accuracy))
    print('*'*50)

plt.figure(figsize=(10, 6))
plt.bar(name, accuracies, color=['blue', 'green', 'red', 'cyan', 'magenta', 'yellow'])
plt.xlabel("Models")
plt.ylabel("Accuracy (%)")
plt.title("Accuracy of Different Models")
plt.ylim([0, 100])
plt.show()

param_grids = {
    'dt': {
        'criterion': ['gini', 'entropy'],
        'max_depth': [None, 5, 10, 15],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'rf': {
        'n_estimators': [100, 200, 300],
        'max_depth': [None, 5, 10, 15],
        'min_samples_split': [2, 5, 10],
        'min_samples_leaf': [1, 2, 4]
    },
    'lg': {
        'penalty': ['l1', 'l2'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100]
    },
    'knn': {
        'n_neighbors': [3, 5, 7, 9],
        'weights': ['uniform', 'distance'],
        'metric': ['euclidean', 'manhattan']
    },
    'svm': {
        'C': [0.1, 1, 10, 100],
        'kernel': ['linear', 'rbf', 'poly'],
        'gamma': ['scale', 'auto', 0.1, 1]
    },
    'nb': {
        'var_smoothing': np.logspace(0,-9, num=100)
    }
}

best_models = {}
for model_name, model in models.items():
  grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], cv=5, scoring='accuracy', n_jobs=-1)
  grid_search.fit(x_train, y_train)
  best_models[model_name] = grid_search.best_estimator_

model_names = list(best_models.keys())
accuracy_scores = []
for model_name, model in best_models.items():
  pred = model.predict(x_test)
  accuracy = accuracy_score(y_test, pred) * 100
  accuracy_scores.append(accuracy)
  print(f'Model: {model_name}')
  print('Accuracy: {:.2f}'.format(accuracy))
  print('*'*50)

plt.figure(figsize=(12, 6))
plt.plot(model_names, accuracy_scores, marker='o', linestyle='-')
plt.xlabel("Model")
plt.ylabel("Best Accuracy")
plt.title("Hyperparameter Tuning Results")
plt.xticks(rotation=45, ha='right')
plt.grid(True)
plt.tight_layout()
plt.show()

